At Capital Group, as a Data Product Manager, I've elevated the landscape of our ETF data and analytics, enhancing the visibility and value of our $31 billion product suite. My role involves bridging the gap between various domains—IT, Sales Compensation, Data Science, Engineering, Consulting, & Business Reporting—by creating synergies that maximize our data's potential.

Started as Data Product Analyst within Insights & Analytics > Promoted to Manager in Enterprise Data Office.


What is your achievements in each of these?


Use of Python & SQL in Databricks
1.	Data Ingestion (Reading and Loading Data)
•	“Used PySpark or Pandas in Python to load data from various sources (CSV, Parquet, databases, or APIs).”
•	Reading ETF Assets from Cloud Storage
2.	Data Transformation (Cleaning & Processing): 
•	“Used Python (Pandas, PySpark) for transformations like filtering, grouping, and cleaning datasets.”
•	Handling Nulls & Renaming Columns
3.	Data Aggregation & Analysis: 
•	“Used grouping and aggregation to analyze investment data.”
•	df_grouped = df_cleaned.groupBy("ETF_Name").agg({"asset_value": "sum"})
•	df_grouped.show()
4.	Data Storage & Performance Optimization: 
•	“I stored transformed data in Delta Tables, which improves query performance.”
5.	Automating Jobs & Integrating with Tableau: 
•	“Used Python to schedule jobs in Databricks.”
•	“Used Databricks SQL to create views that Tableau connects to”

Python Technical Questions
1.	How did you handle large datasets in Python on Databricks?
•	Be ready to explain why you used PySpark instead of Pandas (Pandas struggles with big data).
•	“Since I worked with large datasets, I used PySpark instead of Pandas because it allows distributed computing. I loaded data using Spark DataFrames and optimized queries using cache and partitions.”
2.	How did you optimize your Python code in Databricks?
•	Talk about caching, broadcasting, and using efficient data formats (Parquet, Delta Tables).
•	“I used Spark’s .persist() to store frequently used DataFrames in memory. I also converted CSVs to Parquet for better performance and broadcasted small lookup tables to avoid expensive joins.”
3.	3. What are some key differences between PySpark DataFrames and Pandas DataFrames?
•	“PySpark DataFrames are distributed and can handle large datasets, whereas Pandas DataFrames operate in-memory on a single machine, which limits scalability.”

SQL Technical Questions
1.	How did you use SQL in Databricks?: 
•	“Ran SQL queries within Python using Spark
2.	What performance optimizations did you use for SQL queries?
•	Mention indexing, partitioning, and using Delta Tables.
•	“I partitioned large tables by date to improve query speed, used Delta Tables for faster reads/writes, and optimized joins using ZORDER indexing.”

fundphun@apllo.com
My Core Activities











ETF Data & Pipeline
Core Activities	Data Acquisition	Ingestion Planning	Production	Validation
	Use case analysis	ETL Pipeline	Land as MFT – Security platform	Did our systems operate as intended?  (i.e., preprocessing transformations, block seeding account numbers)
	Requirements gathering	7-8 different systems
	Moved to DataLake – we call Caspian with is on AWS S3 platform	Are we missing data?
	Vendor outreach/communication	6-7 different teams	Sales transactions processed through 4 additional systems	Did the vendor meet SLA?  How long did it take from start to finish?
	Data Quality Check	2 pipelines run simultaneously for Sales and one for Assets	Assets is sent to ADLS, transformed, the published to Data warehouse	Did we duplicate any data along the way?  Ensure all raw untransformed data is delivered to ADLS
Wins	9 ETF Data Sources Integrated Representing $31B In  AUM	867 Files Processed	Over 24 Reports Leverage This Data Internal	36 Dashboard Created for Validation


List of Responsibilities

1.	ETF Data Pipeline Management
a.	Data Acquisition (i.e., vendor management, requirements gathering)
b.	Ingestion planning (i.e., preprocessing transformations)
c.	Data in Motion 
2.	Validation/Analysis
a.	Data extraction, transformation, wrangling, engineering
b.	Creating automated pipelines to validate production pipelines.
c.	SQL, Python, Databricks
3.	Reporting
a.	Turning the results of validation/analysis into displayable insights (i.e., Tableau, PowerBI, Excel Pivot)
b.	Providing management with presentations, an overview of our pipelines, teams involved, upcoming milestones, weaknesses/strengths of the current process, and areas for improvement.
c.	Reports tracking other technical aspects of the pipeline, such as preprocessing transformations (ensuring they’re working), market transparency reports of competitors, and firms where we’re lacking transparency (i.e., access to identifiable data)
4.	Documentation
a.	A well-maintained catalog of all technical, crucial aspects of ETF Data here at Capital Group. 
b.	Tell us about a time when you had to balance competing priorities and manage a challenging project with a tight deadline.
c.	Used for training.
d.	Referenced for other technical and business teams.
e.	Crucial for change management events
Product Manager Core
1.	Someone to manage a sprint backlog; they want a Scrum Product Owner
2.	Oversee and coordinate multiple projects; they want a Program Manager
3.	Develop project plans, timelines, and budgets; they want a Project Manager
4.	All of the above; they want a miracle


